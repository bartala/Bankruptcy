# -*- coding: utf-8 -*-
"""data_analysis*.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-dbCOfwcFsC_BHjN2k4xKafuu6oHGlLk
"""

!pip install vaderSentiment

!pip uninstall rpy2 -y
!pip install rpy2==3.5.1

# Commented out IPython magic to ensure Python compatibility.
# %load_ext rpy2.ipython

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %%R
# lib_loc <- "/content/drive/MyDrive/Colab Notebooks/r-lib"

# Commented out IPython magic to ensure Python compatibility.
# '''
# %%R
# install.packages("data.table", lib =lib_loc)
# install.packages("ggpubr",lib =lib_loc)
# install.packages("DescTools", lib =lib_loc)
# install.packages("NbClust", lib =lib_loc)
# install.packages("factoextra", lib =lib_loc)
# install.packages("gsubfn", lib =lib_loc)
# install.packages("proto", lib =lib_loc)
# install.packages("ggplot2", lib =lib_loc)
# install.packages("RSQLite", lib =lib_loc)
# install.packages("sqldf", lib=lib_loc)
# install.packages("chron",lib= lib_loc)
# install.packages("minpack.lm", lib=lib_loc)
# install.packages("AICcmodavg", lib=lib_loc)
# install.packages("gridExtra",lib=lib_loc)
# install.packages("rsq", lib=lib_loc)
# install.packages("caret", lib=lib_loc)
# install.packages("zoo", lib=lib_loc)
# install.packages("KneeArrower", lib=lib_loc)
# install.packages("dplyr", lib=lib_loc)
# install.packages("nbclust", lib =lib_loc)
# install.packages("Ckmeans.1d.dp", lib =lib_loc)

# Commented out IPython magic to ensure Python compatibility.
# %%R
# library(RSQLite, lib.loc = lib_loc)
# library(proto, lib.loc = lib_loc)
# library(gsubfn, lib.loc = lib_loc)
# library(readr, lib.loc = lib_loc)
# library(ggplot2, lib.loc = lib_loc)
# library(ggpubr, lib.loc = lib_loc)
# library(pROC, lib.loc = lib_loc)
# library(DescTools, lib.loc = lib_loc)
# library(NbClust, lib.loc = lib_loc)
# library(factoextra, lib.loc = lib_loc)
# library(sqldf, lib.loc = lib_loc)
# library(cowplot,lib.loc = lib_loc)
# library(AICcmodavg,lib.loc = lib_loc)
# library(minpack.lm,lib.loc = lib_loc)
# library(gridExtra,lib.loc = lib_loc)
# library(rsq,lib.loc = lib_loc)
# library(caret,lib.loc = lib_loc)
# library(zoo,lib.loc = lib_loc )
# library(broom)
# library(KneeArrower,lib.loc = lib_loc)
# library(googlesheets4,lib.loc = lib_loc)
# library(Ckmeans.1d.dp, lib =lib_loc)

# Commented out IPython magic to ensure Python compatibility.
# %%R
# 
# # before covid data
# PTH1 = "/content/drive/MyDrive/Colab Notebooks/BIU_HEB/data/"

"""### Load datasets of tweets posted before and after the announcment"""

# Commented out IPython magic to ensure Python compatibility.
# %%R
# 
# data <- read_csv(paste0(PTH1,"2012_2022_bankruptcy2.csv"), show_col_types = FALSE)
# 
# data <-data[data$company != 'TailorBrands',]

# read metadata about companies from Google spreadsheet and write to local file
import requests
import pandas as pd

SHEET_ID='1_aQKhRUhpMRHbNdz22ysY8wIdQ-93EJHmk_Vyb04HZE'

r = requests.get(f'https://docs.google.com/spreadsheet/ccc?key={SHEET_ID}&output=csv')
open('dataset.csv', 'wb').write(r.content)
df = pd.read_csv('dataset.csv')

# Commented out IPython magic to ensure Python compatibility.
# # load metadata from local file
# %%R
# 
# metadata <- read_csv('/content/dataset.csv',show_col_types = FALSE)

# Commented out IPython magic to ensure Python compatibility.
# %%R
# 
# companies <- unique(data[data$dataset %in% c("before_covid", "covid"),]$company)
# 
# companies_2 <- unique(data[data$dataset == 'all_2020_new',]$company)
# companies_2 <- companies_2[companies_2 %in% metadata$`Twitter Username`]
# 
# companies <- union(companies,companies_2)
# 
# rm(companies_2)

# Commented out IPython magic to ensure Python compatibility.
# %%R
# 
# data <- data[data$company %in% companies,]

"""### Descriptive statistics and data cleaning"""

# Commented out IPython magic to ensure Python compatibility.
# %%R
# 
# # nuber of unique users in the posts
# paste0("unique users: ", length(unique(data$author_id)) )

# Commented out IPython magic to ensure Python compatibility.
# %%R
# # total number of tweets
# print(
#     paste0("total number of tweets (before and after the announcement): ",
#            nrow(data)
#           )
#       )
# 
# 
# # number of companis
# print(paste0("number of companies: ",length(unique(data$company))) )
# 
# 
# # number of tweets per company
# table(data$company)

# Commented out IPython magic to ensure Python compatibility.
# # split data to before and after the announcement
# %%R
# 
# 
# before <- data[grepl( 'before', data$data, fixed = TRUE),]
# 
# after <- data[grepl( 'after', data$data, fixed = TRUE),]
# 
# 
# x_after <- nrow(after)
# x_before <- nrow(before)
# 
# # delete messages originated by the companies
# 
# after <- after[!after$author_id %in% metadata$UserID,]
# 
# before <- before[!before$author_id %in% metadata$UserID,]
# 
# 
# print( paste0("deleted tweets by companies after: ", x_after - nrow(after) ) )
# print( paste0("deleted tweets by companies before: ", x_before - nrow(before) ) )

"""### Temporal analysis: number of mentions per day and company"""

# Commented out IPython magic to ensure Python compatibility.
# %%R
# 
# before$created_at<-as.character(before$created_at)
# after$created_at<-as.character(after$created_at)
# 
# # keep only day date (no time)
# before$date <- substr(before$created_at,0,10)
# after$date <- substr(after$created_at,0,10)
# 
# temp_before <- sqldf("select company, date, count(1) as freq from before group by company, date")
# 
# temp_before$when<-'before'
# 
# temp_after <-  sqldf("select company, date, count(1) as freq from after group by company, date")
# 
# temp_after$when<-'after'
# 
# tmp <- rbind(temp_before, temp_after)
# 
# tmp <- merge(tmp,metadata, by.x='company', by.y='Twitter Username')
# 
# tmp$number <- as.Date(tmp$date) - as.Date(tmp$`Date of Bankruptcy`,format = "%B %d, %Y") # day diff from bankruptcy (in unix timestamp)
# 
# tmp <- tmp[(tmp$number) < 31 & (tmp$number) > (-31),] # remove tweets more than 30 pre/post announcement
# 
# tmp <- tmp[ tmp$company %in% intersect(tmp[tmp$when == 'before',]$company, tmp[tmp$when == 'after',]$company),] # keep only companies that appear in both the `before` and `after` datasets
# 
# tmp <- tmp[tmp$`Bankruptcy Strategy` == 'Chapter 11',] # keep only chapter 11 bankruptcy
# 
# names(tmp)[6] <- 'Company Name'
# 
# # save temporary file for future analyses
# write.csv(tmp,file='data0.csv', row.names = FALSE)

"""## Average Twitter mention time series of the companies

Plot for all data and companies (before and after announcment)
"""

# Commented out IPython magic to ensure Python compatibility.
# # Maximum curvature cutoff
# %%R
# library(KneeArrower,lib.loc = lib_loc)
# 
# Allcompanies <- sqldf("select avg(freq) as avg, number from tmp group by number",method = "name__class")
# 
# Allcompanies_post <- Allcompanies[Allcompanies$number>0,]
# 
# x = Allcompanies_post$number
# y = Allcompanies_post$avg
# 
# # find the point at which the circle tangent to the curve has the smallest radius
# # https://cran.r-project.org/web/packages/KneeArrower/vignettes/Example.html
# cutoff.point <- findCutoff(x, y, method="curvature", 0.5)
# print(cutoff.point)
# 
# # plot
# plot(x, y, pch=20, col="gray")
# points(cutoff.point, col="red", cex=3, pch=20)

# Commented out IPython magic to ensure Python compatibility.
# # All companies
# 
# %%R
# 
# p1<-  ggplot(Allcompanies, aes(x=number, y=avg)) +
#         geom_point(shape = 1, colour = "black", fill = "black", size = 2, stroke = 2) +
#         #scale_x_continuous(trans = "log10") +
#         scale_y_continuous(trans = "log10") +
#         #geom_line()+
#         geom_smooth(se=FALSE, formula = y ~ x, method = 'loess') +
#         geom_vline(xintercept = 17, linetype=2, size = 1, colour="darkgray") +
#         # pre-announcement mean
#         ggtitle("Average mention time series of all companies") +
#           ylab("Average company mention [log10]") + xlab("Days relative to bankruptcy announcement")+
#         theme_bw()
# 
# 
# ggsave("mean_curve.pdf", plot = p1, width = 6.67, height = 6.67, units = "in")
# p1

"""## Identify High or Low persistence memory of companies that declared bankruptcy

### Compute the average #tweets before announcement AND average #tweets after the buzz fades out $(t \ge 17)$.
"""

# Commented out IPython magic to ensure Python compatibility.
# # mentions(t >= 18) > mean(mentions(t<t0)) --> High persistance
# # mentionds(t > 18) <= mean(mentions(t<t0)) --> Low persistance
# 
# %%R
# 
# # average number of tweets before announcement
# df2 <- sqldf("select company, avg(freq) as avg_freq from tmp where number < 0 group by company") # before
# df2$When = "Before"
# 
# # average number of tweets after the buzz
# df1 <- sqldf("select company, max(freq) as avg_freq from tmp where number > 17 group by company") # after
# df1$When = "After"
# 
# df_1 <- rbind(df1,df2)

# Commented out IPython magic to ensure Python compatibility.
# %%R
# sqldf("select company, sum(freq) as sm, number from tmp where number == -25 group by company, number",method = "name__class")

# Commented out IPython magic to ensure Python compatibility.
# %%R
# (tmp[tmp$`Company Name` == 'Cumulus Media' & tmp$number == -24 ,'freq'])

# Commented out IPython magic to ensure Python compatibility.
# %%R
# 
# df_2 <- merge(df2,df1, by = 'company', all.x = TRUE)
# 
# df_2$When.x <- NULL
# df_2$When.y <- NULL
# 
# names(df_2) <- c("Company", "Before", "After")
# 
# df_2[is.na(df_2$After), 'After'] <- 0

"""### High persistence memory of companies"""

# Commented out IPython magic to ensure Python compatibility.
# # Companies with more mentions (tweets) after the announcement than before the announcment
# # (i.e., they were not forgotten by the public --> High persistence)
# 
# %%R
# unforgotten_comp <- df_1[df_1$company %in% (df_2[df_2$After >= df_2$Before,'Company']),]
# print(unique(unforgotten_comp$company))
# 
# df <- merge(unforgotten_comp[unforgotten_comp$When == 'After', ], unforgotten_comp[unforgotten_comp$When == 'Before', ], by = 'company')
# df$diff <- df$avg_freq.x - df$avg_freq.y
# df1 <- df[,c(1,2,3,6)]
# df2 <- df[,c(1,4,5,6)]
# 
# names(df1)<-c("company","avg_freq","When","diff")
# names(df2)<-c("company","avg_freq","When","diff")
# df <- rbind(df1,df2)
# 
# df <- df[df$company %in% tmp$company,]
# 
# p5<- ggplot(df, aes(x = reorder(company, -diff), y = avg_freq, fill = When)) +
#   geom_bar(stat = "identity", position = "dodge") +
#   theme_bw() +
#   theme(legend.position = c(0.8, 0.8)) +
#   ggtitle("Companies with High persistence level of memory ordered by the \n difference between #mentions after and before bankruptcy") +
#   ylab("Average company mention") +
#   xlab("Company's Twitter handle") +
#   coord_flip() +
#   labs(fill = "") +
#   scale_fill_manual(labels = c("After", "Before"), values = c("#6699CC", "gray32"))
# 
# 
# #rm(df,df1,df2)
# 
# ggsave("SI_High_persistence.pdf", plot = p5, width = 6.67, height = 6.67, units = "in")
# p5

"""
### Low persistence memory of companies
"""

# Commented out IPython magic to ensure Python compatibility.
# %%R
# 
# forgotten_comp <- df_1[df_1$company %in% (df_2[df_2$After < df_2$Before,'Company']),]
# print(unique(forgotten_comp$company))
# 
# 
# df <- merge(forgotten_comp[forgotten_comp$When == 'After', ], forgotten_comp[forgotten_comp$When == 'Before', ], by = 'company', all.y= TRUE)
# df[is.na(df$avg_freq.x),'avg_freq.x'] <- 0
# df[is.na(df$avg_freq.y),'avg_freq.y'] <- 0
# df$When.x  <- "After"
# 
# df$diff <- df$avg_freq.y - df$avg_freq.x
# df1 <- df[,c(1,2,3,6)]
# df2 <- df[,c(1,4,5,6)]
# 
# names(df1)<-c("company","avg_freq","When","diff")
# names(df2)<-c("company","avg_freq","When","diff")
# df <- rbind(df1,df2)
# 
# 
# # bar plot
# p6<- ggplot(df, aes(x = reorder(company, -diff), y = avg_freq, fill = When)) +
#               geom_bar(stat = "identity", position = "dodge") +
#               theme_bw()+
#               theme( legend.position = c(0.8, 0.8)) +
#               ggtitle("Companies with Low persistence level of memory ordered by the \n difference between #mentions after and before bankruptcy") +
#               ylab("Average company mention") +
#               xlab("Company's Twitter handle") +
#               coord_flip() +
#               labs(fill = "") +
#               scale_fill_manual(labels = c("After", "Before"),values = c("#6699CC", "gray32"))
# 
# 
# #rm(df,df1,df2)
# 
# ggsave("SI_Low_persistence.pdf", plot = p6, width = 6.67, height = 6.67, units = "in")
# p6

"""## Models fitting

### GOF help functions
"""

# Commented out IPython magic to ensure Python compatibility.
# %%R
# 
# # AIC
# gof <- function(model){
# 
#     return(
#               glance(model) %>%
#                   dplyr::select(adj.r.squared, sigma, AIC, BIC, p.value)
#     )
# 
# }
# 
# 
# 
# # R^2
# r_sqared <- function(model,y){
#                     # Calculate R-squared
#                     y_pred <- predict(model) # predicted values
# 
#                     ss_total <- sum((y - mean(y))^2)
#                     ss_residual <- sum((y - y_pred)^2)
# 
#                     r_squared <- (1 - ss_residual/ss_total) # R-squared
#                     return(r_squared)
# }

"""### High persistence memory of companies"""

# Commented out IPython magic to ensure Python compatibility.
# # Unforgotten companies (have high persistence memory)
# 
# %%R
# 
# Unforgotten_companies <- unique(unforgotten_comp$company)
# 
# Unforgotten <- tmp[ (tmp$company) %in% (Unforgotten_companies) & tmp$number >=0, ] # number:= days before/after bankruptcy announcement
# 
# Unforgotten <- sqldf("select avg(freq) as avg, number from Unforgotten group by number",method = "name__class")

# Commented out IPython magic to ensure Python compatibility.
# # models
# %%R
# 
# Unforgotten$x = Unforgotten$number
# Unforgotten$y = Unforgotten$avg
# 
# # Exponential
# expmodel_uf <- lm(y~exp(x), data = Unforgotten)
# print(summary(expmodel_uf))
# 
# # Log
# logmodel_uf <- lm(y~log(x+0.01), data = Unforgotten)
# print(summary(logmodel_uf))
# 
# # Hyperbolic
# hyper_fit_uf <- nlsLM(y ~ a/(1 + b * x), data = Unforgotten, start = list(a = 1, b = 1))
# print(summary(hyper_fit_uf))
# 
# # Biexponential
# biexp_fit_uf <- nlsLM(y ~ a1*exp(-b1*x) + a2*exp(-b2*x), data = Unforgotten, start = c(a1 = 5, b1 = 2, a2 = 3, b2 = 0.5))
# print(summary(biexp_fit_uf))
# 
# # Fit a power law curve using nls
# power_fit_uf <- nls(y ~ a * (x+0.0001)^(-b), data = Unforgotten, start = list(a = 1, b = 1))
# summary(power_fit_uf)

# Commented out IPython magic to ensure Python compatibility.
# # AIC
# %%R
# 
# print(paste0("AIC exp: ", gof(expmodel_uf)$AIC))
# print(paste0("AIC log: ",gof(logmodel_uf)$AIC))
# print(paste0("AIC hypr: ",AIC(hyper_fit_uf)))
# print(paste0("AIC biex: ",AIC(biexp_fit_uf)))
# print(paste0("AIC power: ",AIC(power_fit_uf)))

# Commented out IPython magic to ensure Python compatibility.
# # clac R^2 for non-linear models
# %%R
# 
# print(paste0("R^2 exp: ",   round(r_sqared(expmodel_uf,Unforgotten$y),2)))
# print(paste0("R^2 log: ",   round(r_sqared(logmodel_uf,Unforgotten$y),2)))
# print(paste0("R^2 hypr: ",  round(r_sqared(hyper_fit_uf,Unforgotten$y),2)))
# print(paste0("R^2 biex: ",  round(r_sqared(biexp_fit_uf,Unforgotten$y),2)))
# print(paste0("R^2 power: ", round(r_sqared(power_fit_uf,Unforgotten$y),2)))

# Commented out IPython magic to ensure Python compatibility.
# %%R
# 
# p2<-ggplot(Unforgotten, aes(x=number, y=avg)) +
#             geom_point(shape = 1, colour = "black", fill = "black", size = 2, stroke = 1) +
#             #scale_x_continuous(trans = "log10", limits = c(1, 30)) +
#             scale_y_continuous(trans = "log10", limits = c(28, 400)) +
#             geom_smooth(method="lm", aes(color="Exponential"), formula= (y ~ exp(x)), se=FALSE, linetype = 1) +
#             geom_smooth(method = "glm", aes(color="Log"), formula = y~x, se=FALSE, method.args = list(family = gaussian(link = 'log')),linetype = 1) +
#             geom_smooth(method = "nls", aes(color = "Hyperbolic"), formula = y ~ a/(1 + b * x), start = list(a = 1, b = 1), se = FALSE, linetype = 1) +
#             geom_smooth(method = "nls", aes(color = "Power-law"), formula = y ~ a * (x+0.0000001)^(-b), se = FALSE, method.args = list(start = list(a = 1, b = 1))) +
#             geom_smooth(method = "nls", aes(color = "Biexponential"), formula = y ~ a1*exp(-b1*x) + a2*exp(-b2*x), se = FALSE, method.args = list(start = c(a1 = 5, b1 = 2, a2 = 3, b2 = 0.5))) +
#             geom_smooth(method = "nls", aes(color = "Two-phaze"), formula = y ~ a1*exp^(-b1*x) + a2*x^(-b2), se = FALSE, method.args = list(start = c(a1 = 5, b1 = 2, a2 = 3, b2 = 0.5))) +
#             ggtitle("Unforgotten Companies (log-log scale)") +
#               ylab("Average company mention [log10]") + xlab("Days since the announcement")+
#             annotate("text", x=3, y=255,  label= paste0("AIC Biexponential: ",round(AIC(biexp_fit_uf),2),("; R^2 = ") ,round(r_sqared(biexp_fit_uf,Unforgotten$y),2)),hjust = 0)+
#             annotate("text", x=3, y=215, label= paste0("AIC Exponential: ", round(gof(expmodel_uf)$AIC,2)),hjust = 0) +
#             annotate("text", x=3, y=182, label= paste0("AIC Hyperbolic: ",round(AIC(hyper_fit_uf),2)),hjust = 0)+
#             annotate("text", x=3, y=155, label= paste0("AIC Log: ",round(gof(logmodel_uf)$AIC,2)),hjust = 0)+
#             annotate("text", x=3, y=130, label= paste0("AIC Power-law: ",round(AIC(power_fit_uf),2)),hjust = 0)+
#             theme_bw()
# 
# 
# ggsave("Unforgotten_models.pdf", plot = p2, width = 6.67, height = 6.67, units = "in")
# p2

"""### Low persistence memory of companies"""

# Commented out IPython magic to ensure Python compatibility.
# # Forgotten companies
# 
# %%R
# forgotten_companies <- unique(forgotten_comp$company)
# 
# forgotten <- tmp[ (tmp$company) %in% (forgotten_companies) & tmp$number >=0, ] # number:= number of days since bankruptcy announcement
# 
# forgotten <- sqldf("select avg(freq) as avg, number from forgotten group by number",method = "name__class")

# Commented out IPython magic to ensure Python compatibility.
# # models
# %%R
# 
# forgotten$x = forgotten$number
# forgotten$y = forgotten$avg
# 
# # Exponential
# expmodel_f <- lm(y~exp(x), data = forgotten)
# print(summary(expmodel_f))
# 
# # Log
# logmodel_f <- lm(y~log(x+0.01), data = forgotten)
# print(summary(logmodel_f))
# 
# # Hyperbolic
# hyper_fit_f <- nlsLM(y ~ a/(1 + b * x), data = forgotten, start = list(a = 1, b = 1))
# print(summary(hyper_fit_f))
# 
# # Biexponential
# biexp_fit_f <- nlsLM(y ~ a1*exp(-b1*x) + a2*exp(-b2*x), data = forgotten, start = c(a1 = 5, b1 = 2, a2 = 3, b2 = 0.5))
# print(summary(biexp_fit_f))
# 
# # Fit a power law curve using nls
# power_fit_f <- nls(y ~ a * (x+0.0001)^(-b), data = forgotten, start = list(a = 1, b = 1))
# summary(power_fit_f)

# Commented out IPython magic to ensure Python compatibility.
# %%R
# print(paste0("AIC exp: ", gof(expmodel_f)$AIC))
# print(paste0("AIC log: ",gof(logmodel_f)$AIC))
# print(paste0("AIC hypr: ",AIC(hyper_fit_f)))
# print(paste0("AIC biex: ",AIC(biexp_fit_f)))
# print(paste0("AIC power: ",AIC(power_fit_f)))

# Commented out IPython magic to ensure Python compatibility.
# %%R
# 
# p3<-ggplot(forgotten, aes(x=number, y=avg)) +
#             geom_point(shape = 1, colour = "black", fill = "black", size = 2, stroke = 1) +
#             #scale_x_continuous(trans = "log10", limits = c(1, 30)) +
#             scale_y_continuous(trans = "log10", limits = c(28, 400)) +
#             #coord_cartesian(ylim = c(0,500)) +
#             geom_smooth(method="lm", aes(color="Exponential"), formula= (y ~ exp(x)), se=FALSE, linetype = 1) +
#             geom_smooth(method = "glm", aes(color="Log"), formula = y~x, se=FALSE, method.args = list(family = gaussian(link = 'log')),linetype = 1) +
#             geom_smooth(method = "nls", aes(color = "Hyperbolic"), formula = y ~ a/(1 + b * x), start = list(a = 1, b = 1), se = FALSE, linetype = 1) +
#             geom_smooth(method = "nls", aes(color = "Power-law"), formula = y ~ a * (x+0.0000001)^(-b), se = FALSE, method.args = list(start = list(a = 1, b = 1))) +
#             geom_smooth(method = "nls", aes(color = "Biexponential"), formula = y ~ a1*exp(-b1*x) + a2*exp(-b2*x), se = FALSE, method.args = list(start = c(a1 = 5, b1 = 2, a2 = 3, b2 = 0.5))) +
#             ggtitle("Forgotten Companies") +
#               ylab("Average company mention [log10]") + xlab("Days since the announcement")+
#             annotate("text", x=4, y=265,  label= paste0("AIC Biexponential: ",round(AIC(biexp_fit_f),2),"; ",expression(R^2), "= " ,round(r_sqared(biexp_fit_f,forgotten$y),2)),hjust = 0)+
#             annotate("text", x=4, y=225, label= paste0("AIC Exponential: ", round(gof(expmodel_f)$AIC,2)),hjust = 0) +
#             annotate("text", x=4, y=192, label= paste0("AIC Hyperbolic: ",round(AIC(hyper_fit_f),2)),hjust = 0)+
#             annotate("text", x=4, y=165, label= paste0("AIC Log: ",round(gof(logmodel_f)$AIC,2)),hjust = 0)+
#             annotate("text", x=4, y=140, label= paste0("AIC Power-law: ",round(AIC(power_fit_f),2)),hjust = 0)+
#             theme_bw()
# 
# ggsave("forgotten_models.pdf", plot = p3, width = 6.67, height = 6.67, units = "in")
# p3

"""## Inter tweet time"""

# Commented out IPython magic to ensure Python compatibility.
# %%R
# data <- read_csv(paste0(PTH1,"2012_2022_bankruptcy2.csv"), show_col_types = FALSE)
# 
# 
# # split data into before and after the announcement
# before <- data[grepl( 'before', data$data, fixed = TRUE),]
# after <- data[grepl( 'after', data$data, fixed = TRUE),]
# 
# # delete messages originated by the companies
# after <- after[!after$author_id %in% metadata$UserID,]
# before <- before[!before$author_id %in% metadata$UserID,]
# 
# 
# 
# # keep only companies that we want to analyze
# after<-after[after$company %in% tmp$company,]
# before<-before[before$company %in% tmp$company,]

# Commented out IPython magic to ensure Python compatibility.
# %%R
# # before High persistence (unforgotten)
# before_unforgot <- before[before$company %in% (Unforgotten_companies),]
# my_datetime <- as.POSIXct(before_unforgot$created_at, format="%Y-%m-%d %H:%M:%S", tz="UTC")
# my_datetime <- my_datetime[order(as.Date(my_datetime), decreasing = TRUE)]
# my_datetime <- cbind(my_datetime[1:(length(my_datetime)-1)], my_datetime[2:(length(my_datetime))])
# 
# # inter Tweet time
# inter_Tweet_time_before_unforgot <- data.frame(my_datetime[,1] - my_datetime[,2])
# names(inter_Tweet_time_before_unforgot) <- "diff_before_unforgot"

# Commented out IPython magic to ensure Python compatibility.
# %%R
# # before Low persistence (forgotten)
# before_forgot <- before[before$company %in% (forgotten_companies),]
# my_datetime <- as.POSIXct(before_forgot$created_at, format="%Y-%m-%d %H:%M:%S", tz="UTC")
# my_datetime <- my_datetime[order(as.Date(my_datetime), decreasing = TRUE)]
# my_datetime <- cbind(my_datetime[1:(length(my_datetime)-1)], my_datetime[2:(length(my_datetime))])
# 
# # inter Tweet time
# inter_Tweet_time_before_forgot <- data.frame(my_datetime[,1] - my_datetime[,2])
# names(inter_Tweet_time_before_forgot) <- "diff_before_forgot"

# Commented out IPython magic to ensure Python compatibility.
# %%R
# library(ggplot2)
# 
# # Create example data with two vectors of unequal lengths
# set.seed(123)
# x <- inter_Tweet_time_before_forgot$diff_before_forgot
# y <- inter_Tweet_time_before_unforgot$diff_before_unforgot
# 
# # Merge the two vectors into a single data frame
# df <- data.frame(Inter_Tweet_Time = c(x, y),
#                  Group = factor(rep(c("Low", "High"), c(length(x), length(y)))))
# 
# # Create ECDF plot
# p4 <- ggplot(df, aes(Inter_Tweet_Time, color = Group)) +
#           stat_ecdf(size = 1) +
#           scale_x_log10() +
#           #scale_y_log10() +
#           ggtitle("ECDF of inter-tweet time before bankruptcy announcement. \n The tweets mention forgotten and unforgotten companies") +
#             ylab(expression(P(X<x))) + xlab("Inter-tweet time [Seconds]")+
#           annotate("text", x=1e+3, y=0.5, label= paste0("Two-sample Kolmogorov-Smirnov test \n", "D-statistic = 0.37, p-value < 2.2e-16"),hjust = 0) +
#           theme_bw()
# 
# 
# ggsave("ecdf.pdf", plot = p4, width = 6.67, height = 6.67, units = "in")
# p4
#

"""To test H1 and reveal whether forgotten and unforgotten companies have different temporal inforamtion spreading patterns, we compare the distribution of the ECDFs by using the Kolmogorov-Smirnov (KS) D-statistic test.
The D-statistic is defined as the maximum distance: D = max(|F1(x) âˆ’ F2(x)|), where x represents the range of the random variable, and F1 and F2 represent the empirical cumulative distributions functions.
The smaller the distance, the more similar the distribution curves and, hence, the more likely are the two samples to come from the same distribution.
In the KS-test, a p_value < 0.05 indicates that the samples are **not** drawn from the same distribution
"""

# Commented out IPython magic to ensure Python compatibility.
# # KS-test
# %%R
# 
# ks.test(x, y)

"""## Generate feture for logistic regression


1. **avg_time_from_first** - Time difference from the posting of the FIRST tweet about the company.
  * Captures bursty user interactions [59, 60], which can explain contagion spread [61].

2. **Pre-announcement mean**: Arithmetic mean of number of mentions in days $t \in [-30, 0]$, prior to bankruptcy announcement.


3. **Followers**


4. **Short-term boost**: Maximum mentions during days $t=0$ through $t=6$ following bankruptcy announcement, minus the pre-announcement mean.

5. **Long-term boost**

6. **number of tweets before**

7. **average tweet length**

8. **average tweet sentiment**

#### data preperation
"""

# Commented out IPython magic to ensure Python compatibility.
# # help functions
# %%R
# 
# # histogram and statistics
# histme<-function(x, xaxis_title=""){
#                 if(xaxis_title != ""){
#                         pdf(paste0("histogram_",xaxis_title,".pdf"))
#                         hist(x, prob = TRUE, breaks = 50, xlab = xaxis_title)
#                         lines(density(x), col = "blue", lwd = 3)
#                         dev.off()
#                 }
#                         print(paste0("mean: ",mean(x)))
# 
#                         print(paste0("mean CI: ", t.test(x)$conf.int))
# 
#                         MedianCI(x,
#                                 conf.level = 0.95,
#                                 na.rm = FALSE,
#                                 method = "exact",
#                                 R = 10000)
# }
# 
# 
# 
# 
# 
# # remove outliers and recalulate the Pre-announcement mean (PAM)
# 
# PAM_remove_outliers <- function(x){
# 
#     # Identify and remove outliers
#     outliers <- boxplot(x, plot=FALSE)$out
#     x_no_outliers <- x[!x %in% outliers]
#     return(mean(x_no_outliers))
# 
# }

# Commented out IPython magic to ensure Python compatibility.
# # 1) Average time difference betweeb the posting time of a tweet and the posting of the FIRST tweet about the company
# 
# %%R
# 
# before <- before %>% group_by(company) %>% mutate(min_value = min(created_at)) # find the time of the first post
# 
# before$time_from_first <- as.Date(before$created_at) - as.Date(before$min_value)
# 
# result <- sqldf("select company, avg(time_from_first) as avg_time_from_first from before group by company")
#

# Commented out IPython magic to ensure Python compatibility.
# # 2) Pre-announcement mean
# %%R
# 
# # create empty dataframe
# Pre_announcement_mean <- data.frame()
# Pre_announcement_mean$company = character()
# Pre_announcement_mean$Pre_announcement_mean = numeric()
# 
# for(comp in unique(tmp$company)){
# 
#     x <- tmp[tmp$company == comp & tmp$number<0,'freq']
#     mean_PAM <- PAM_remove_outliers(x)
#     Pre_announcement_mean <- rbind( Pre_announcement_mean,
#                                     data.frame(
#                                                   company = comp,
#                                                   Pre_announcement_mean = mean_PAM
#                                                 )
#                                       )
# }
# 
# #Pre_announcement_mean = sqldf("select avg(freq) as Pre_announcement_mean, company from tmp where number<0 group by company")
# 
# result$Pre_announcement_mean <- NULL
# result <- merge(result, Pre_announcement_mean, by = 'company')
# 
# histme(result[result$Pre_announcement_mean>0,'Pre_announcement_mean'])

# Commented out IPython magic to ensure Python compatibility.
# # 3) Followers
# 
# %%R
# 
# df <- metadata[,c('Company','Twitter Username','Followers', 'Following','Avg_Google_Trends')]
# 
# result <- merge(result, df,  by.x = 'company', by.y = 'Twitter Username')

# Commented out IPython magic to ensure Python compatibility.
# # 4) Short-term boost
# 
# %%R
# 
# Short_term_boost <- sqldf("select max(freq) as shrt_boost, company from tmp where number between -1 and 17 group by company")
# 
# result$shrt_boost<-NULL
# 
# result <- merge(result, Short_term_boost, by = 'company')
# 
# result$shrt_boost <- result$shrt_boost - result$Pre_announcement_mean
# 
# histme(result$shrt_boost, "")

# Commented out IPython magic to ensure Python compatibility.
# # 5) Long-term Boost
# %%R
# 
# Long_term_boost <- sqldf("select max(freq) as lng_boost, company from tmp where `when`='after'and number > 17 group by company")
# 
# 
# result$lng_boost<-NULL
# result <- merge(result, Long_term_boost, by = 'company', all.x = TRUE)
# 
# result[is.na(result$lng_boost),'lng_boost'] <- 0
# 
# result$lng_boost<-result$lng_boost - result$Pre_announcement_mean
# 
# histme(result[!is.na(result$lng_boost),'lng_boost'], "Long-term Boost")

# Commented out IPython magic to ensure Python compatibility.
# # 6) number of tweets before
# %%R
# 
# df <- sqldf("select sum(freq) as number_of_tweets_before, company from tmp where number < 0 group by company")
# 
# result$number_of_tweets_before<-NULL
# result <- merge(result, df, by = 'company', all.x = TRUE)

# Commented out IPython magic to ensure Python compatibility.
# # 7) Average tweet length
# %%R
# 
# before$txt_length <- nchar(before$text)
# 
# df = sqldf("select company, avg(txt_length) as avg_txt_len from before group by company")
# 
# result$avg_txt_len<-NULL
# result <- merge(result, df, by = 'company', all.x = TRUE)

# Commented out IPython magic to ensure Python compatibility.
# 8) Average tweet sentiment
# %R -o before

# Get the sentiment score of a text vector

from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

analyzer = SentimentIntensityAnalyzer()

sentiment_scores = []
sentiment_pos = []
sentiment_neg = []
sentiment_neu = []

for text in before['text']:
  scores = analyzer.polarity_scores(text)
  sentiment_scores.append( scores['compound'] )
  sentiment_neg.append( scores['neg'] )
  sentiment_pos.append( scores['pos'] )
  sentiment_neu.append( scores['neu'] )

before['sentiment'] = sentiment_scores
before['pos'] = sentiment_pos
before['neg'] = sentiment_neg
before['neu'] = sentiment_neu

# Commented out IPython magic to ensure Python compatibility.
# %R -i before

# Commented out IPython magic to ensure Python compatibility.
# # 8) Average tweet sentiment
# %%R
# 
# sentiment = sqldf("select company, avg(sentiment) as avg_sentiment,avg(pos) as avg_pos,avg(neg) as avg_neg,avg(neu) as avg_neu from before group by company")
# 
# result <- merge(result, sentiment, by = 'company', all.x = TRUE)

# Commented out IPython magic to ensure Python compatibility.
# # 9) public or private company
# %%R
# 
# result <- merge(result, metadata[,c('Twitter Username','Private_Public')], by.x = 'company' , by.y =  'Twitter Username', all.x = TRUE)
# 
# result$Private_Public <- tolower(result$Private_Public)
# 
# #result[result$Private_Public == 'public pb', 'Private_Public'] <- 'public' # taken private post-bankruptcy

"""## Sentiment analysis

https://www.geeksforgeeks.org/python-sentiment-analysis-using-vader/

The Compound score is a metric that calculates the sum of all the lexicon ratings which have been normalized between -1(most extreme negative) and +1 (most extreme positive).


**How vader sentiment analyzer works:**

https://www.geeksforgeeks.org/facebook-sentiment-analysis-using-python/

VADER uses a combination of A sentiment lexicon which is a list of lexical features (e.g., words) which are generally labeled according to their semantic orientation as either positive or negative.
sentiment analyzer not only tells about the Positivity and Negativity score but also tells us about how positive or negative a sentiment is.
Then, We used the polarity_scores() method to obtain the polarity indices for the given sentence.
"""

# help function

from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

def dentiment_analyzer(data):

  analyzer = SentimentIntensityAnalyzer()

  sentiment_scores = []
  sentiment_pos = []
  sentiment_neg = []
  sentiment_neu = []

  for text in data['text']:
    scores = analyzer.polarity_scores(text)
    sentiment_scores.append( scores['compound'] )
    sentiment_neg.append( scores['neg'] )
    sentiment_pos.append( scores['pos'] )
    sentiment_neu.append( scores['neu'] )

  data['sentiment'] = sentiment_scores
  data['neg'] = sentiment_neg
  data['pos'] = sentiment_pos
  data['neu'] = sentiment_neu
  return(data)

"""### Unforgotten compamies: sentiment after announcment $t>0$"""

# Commented out IPython magic to ensure Python compatibility.
# %%R
# 
# after_unforgot <- data[data$company %in% Unforgotten_companies,]
# 
# after_unforgot$created_at<-as.character(after_unforgot$created_at)
# 
# # keep only day date (no time)
# after_unforgot$date <- substr(after_unforgot$created_at,0,10)
# 
# after_unforgot <- merge(after_unforgot,metadata[,c('Twitter Username','Private_Public', 'Date of Bankruptcy')], by.x='company', by.y='Twitter Username')
# 
# after_unforgot$number <- as.Date(after_unforgot$date) - as.Date(after_unforgot$`Date of Bankruptcy`,format = "%B %d, %Y") # day diff from bankruptcy (in unix timestamp)
# 
# after_unforgot <- after_unforgot[after_unforgot$number >= 0, ]

# Commented out IPython magic to ensure Python compatibility.
# 8) Average tweet sentiment
# %R -o after_unforgot

# Get the sentiment score of a text vector

after_unforgot = dentiment_analyzer(after_unforgot)

# Commented out IPython magic to ensure Python compatibility.
# %R -i after_unforgot

# Commented out IPython magic to ensure Python compatibility.
# %%R
# 
# a_uf <- sqldf("select company, number, avg(sentiment) as avg_sent, avg(pos) as avg_sent_pos, avg(neg) as avg_sent_neg, avg(neu) as avg_sent_neu from after_unforgot group by company, number")

"""### Forgotten compamies: sentiment after announcment $t>0$"""

# Commented out IPython magic to ensure Python compatibility.
# %%R
# 
# after_forgot <- data[data$company %in% forgotten_companies,]
# 
# after_forgot$created_at<-as.character(after_forgot$created_at)
# 
# # keep only day date (no time)
# after_forgot$date <- substr(after_forgot$created_at,0,10)
# 
# after_forgot <- merge(after_forgot,metadata[,c('Twitter Username','Private_Public', 'Date of Bankruptcy')], by.x='company', by.y='Twitter Username')
# 
# after_forgot$number <- as.Date(after_forgot$date) - as.Date(after_forgot$`Date of Bankruptcy`,format = "%B %d, %Y") # day diff from bankruptcy (in unix timestamp)
# 
# after_forgot <- after_forgot[after_forgot$number >= 0, ]

# Commented out IPython magic to ensure Python compatibility.
# Average tweet sentiment
# %R -o after_forgot

# Get the sentiment score of a text vector

after_forgot = dentiment_analyzer(after_forgot)

# Commented out IPython magic to ensure Python compatibility.
# %R -i after_forgot

# Commented out IPython magic to ensure Python compatibility.
# %%R
# 
# a_f <- sqldf("select company, number, avg(sentiment) as avg_sent, avg(pos) as avg_sent_pos, avg(neg) as avg_sent_neg, avg(neu) as avg_sent_neu from after_forgot group by company, number")

"""### Boxplot of High and Low persistence sentiment

### before bankruptcy
"""

# Commented out IPython magic to ensure Python compatibility.
# %%R
# 
# # -------------------  Unforgotten -----------------------------------------------
# 
# before_unforgot <- data[data$company %in% Unforgotten_companies,]
# 
# before_unforgot$created_at<-as.character(before_unforgot$created_at)
# 
# # keep only day date (no time)
# before_unforgot$date <- substr(before_unforgot$created_at,0,10)
# 
# before_unforgot <- merge(before_unforgot,metadata[,c('Twitter Username','Private_Public_Clean', 'Date of Bankruptcy')], by.x='company', by.y='Twitter Username')
# 
# before_unforgot$number <- as.Date(before_unforgot$date) - as.Date(before_unforgot$`Date of Bankruptcy`,format = "%B %d, %Y") # day diff from bankruptcy (in unix timestamp)
# 
# before_unforgot <- before_unforgot[before_unforgot$number < 0, ]
# 
# 
# # -------------------  Forgotten -----------------------------------------------
# 
# 
# before_forgot <- data[data$company %in% forgotten_companies,]
# 
# before_forgot$created_at<-as.character(before_forgot$created_at)
# 
# # keep only day date (no time)
# before_forgot$date <- substr(before_forgot$created_at,0,10)
# 
# before_forgot <- merge(before_forgot,metadata[,c('Twitter Username','Private_Public', 'Date of Bankruptcy')], by.x='company', by.y='Twitter Username')
# 
# before_forgot$number <- as.Date(before_forgot$date) - as.Date(before_forgot$`Date of Bankruptcy`,format = "%B %d, %Y") # day diff from bankruptcy (in unix timestamp)
# 
# before_forgot <- before_forgot[before_forgot$number < 0, ]

# Commented out IPython magic to ensure Python compatibility.
# Average tweet sentiment
# %R -o before_unforgot

# Commented out IPython magic to ensure Python compatibility.
# Average tweet sentiment
# %R -o before_forgot

# Get the sentiment score of a text vector

before_unforgot = dentiment_analyzer(before_unforgot)

#---- before_forgot --------
# Get the sentiment score of a text vector
before_forgot = dentiment_analyzer(before_forgot)

# Commented out IPython magic to ensure Python compatibility.
# %R -i before_unforgot

# Commented out IPython magic to ensure Python compatibility.
# %R -i before_forgot

# Commented out IPython magic to ensure Python compatibility.
# %%R
# # Low
# b_f <- sqldf("select company, number, avg(sentiment) as avg_sent, avg(pos) as avg_sent_pos, avg(neg) as avg_sent_neg, avg(neu) as avg_sent_neu from before_forgot group by company, number")
# 
# # High
# b_uf <- sqldf("select company, number, avg(sentiment) as avg_sent, avg(pos) as avg_sent_pos, avg(neg) as avg_sent_neg, avg(neu) as avg_sent_neu from before_unforgot group by company, number")

"""### Boxplots"""

# Commented out IPython magic to ensure Python compatibility.
# %%R
# 
# # combine High and Low into a single data.frame.
# b_f$Persistence_t = "before_Low"
# b_f$Persistence = "Low"
# 
# b_uf$Persistence_t = "before_High"
# b_uf$Persistence = "High"
# 
# a_f$Persistence_t = "after_Low"
# a_f$Persistence = "Low"
# 
# a_uf$Persistence_t = "after_High"
# a_uf$Persistence = "High"
# 
# b <- rbind(b_f, b_uf, a_f, a_uf)

# Commented out IPython magic to ensure Python compatibility.
# %%R
# 
# #-------------- Low persistence --------------------------------------
# pdf(file="Low_Sentiment_Boxplot.pdf")
#     boxplot(
#         b[b$Persistence_t == 'before_Low', 'avg_sent_pos'],
#         b[b$Persistence_t == 'after_Low', 'avg_sent_pos'],
#         b[b$Persistence_t == 'before_Low', 'avg_sent_neg'],
#         b[b$Persistence_t == 'after_Low', 'avg_sent_neg'],
#         b[b$Persistence_t == 'before_Low', 'avg_sent_neu'],
#         b[b$Persistence_t == 'after_Low', 'avg_sent_neu'],
#         b[b$Persistence_t == 'before_Low', 'avg_sent'],
#         b[b$Persistence_t == 'after_Low', 'avg_sent'],
#         names = c('Pos B', 'Pos A', 'Neg B', 'Neg A', 'Neu B', 'Neu A','All B', 'All A'),
#         ylab ='Sentiment',
#         main='Sentiment of Events with Low Persistence',
#         col = c("red","lightblue")
#         )
#     legend("topleft", c("After (A)","Before (B)"), border=c("lightblue", "red"), fill = c("lightblue", "red"))
#         # Add sample size on top
#     text(
#           x=c(1.5, 3.5, 5.5,7.5),
#           y= c(-0.1, -0.25, 0.2, -0.1),
#           c("P-value = 5.921e-04",
#             "P-value = 0.018 \n Delta = 119%",
#             "P-value = 0.1086 \n Delta = 100.69%",
#             "P-value = 1.198e-03"),
#          cex = 0.9,
#         )
# dev.off()
# 
# 
# #-------------- High persistence --------------------------------------
# pdf(file="High_Sentiment_Boxplot.pdf")
#     boxplot(
#             b[b$Persistence_t == 'before_High', 'avg_sent_pos'],
#             b[b$Persistence_t == 'after_High', 'avg_sent_pos'],
#             b[b$Persistence_t == 'before_High', 'avg_sent_neg'],
#             b[b$Persistence_t == 'after_High', 'avg_sent_neg'],
#             b[b$Persistence_t == 'before_High', 'avg_sent_neu'],
#             b[b$Persistence_t == 'after_High', 'avg_sent_neu'],
#             b[b$Persistence_t == 'before_High', 'avg_sent'],
#             b[b$Persistence_t == 'after_High', 'avg_sent'],
#             names = c('Pos B', 'Pos A', 'Neg B', 'Neg A', 'Neu B', 'Neu A','All B', 'All A'),
#             ylab ='Sentiment',
#             main='Sentiment of Events with High Persistence',
#             col = c("red","lightblue")
#             )
#     legend("topleft", c("After (A)","Before (B)"), border=c("lightblue", "red"), fill = c("lightblue", "red"))
#         text(
#           x=c(1.5, 3.5, 5.5,7.5),
#           y= c(-0.1, -0.25, 0.2, -0.1),
#           c("P-value = 3.039e-06",
#             "P-value = 3.297e-05 \n Delta = 117.17%",
#             "P-value = 0.01256 \n Delta = 101.02%",
#             "P-value = 4.832e-08"),
#          cex = 0.9,
#         )
# dev.off()

# Commented out IPython magic to ensure Python compatibility.
# %%R
# 
# 
# boxplot(
#         b[b$Persistence_t == 'before_Low', 'avg_sent_pos'],
#         b[b$Persistence_t == 'after_Low', 'avg_sent_pos'],
#         b[b$Persistence_t == 'before_Low', 'avg_sent_neg'],
#         b[b$Persistence_t == 'after_Low', 'avg_sent_neg'],
#         b[b$Persistence_t == 'before_Low', 'avg_sent_neu'],
#         b[b$Persistence_t == 'after_Low', 'avg_sent_neu'],
#         b[b$Persistence_t == 'before_Low', 'avg_sent'],
#         b[b$Persistence_t == 'after_Low', 'avg_sent'],
#         names = c('Pos B', 'Pos A', 'Neg B', 'Neg A', 'Neu B', 'Neu A','All B', 'All A'),
#         ylab ='Sentiment',
#         main='Sentiment of Events with Low Persistence',
#         col = c("red","lightblue")
#         )
#     legend("topleft", c("After (A)","Before (B)"), border=c("lightblue", "red"), fill = c("lightblue", "red"))
#     # Add sample size on top
#     text(
#           x=c(1.5, 3.5, 5.5,7.5),
#           y= c(-0.1, -0.25, 0.2, -0.1),
#           c("P-value = 5.921e-04",
#             "P-value = 0.018 \n Delta = 119%",
#             "P-value = 0.1086 \n Delta = 100.69%",
#             "P-value = 1.198e-03"),
#          cex = 0.9,
#         )
# 
# 
# 
#   boxplot(
#         b[b$Persistence_t == 'before_High', 'avg_sent_pos'],
#         b[b$Persistence_t == 'after_High', 'avg_sent_pos'],
#         b[b$Persistence_t == 'before_High', 'avg_sent_neg'],
#         b[b$Persistence_t == 'after_High', 'avg_sent_neg'],
#         b[b$Persistence_t == 'before_High', 'avg_sent_neu'],
#         b[b$Persistence_t == 'after_High', 'avg_sent_neu'],
#         b[b$Persistence_t == 'before_High', 'avg_sent'],
#         b[b$Persistence_t == 'after_High', 'avg_sent'],
#         names = c('Pos B', 'Pos A', 'Neg B', 'Neg A', 'Neu B', 'Neu A','All B', 'All A'),
#         ylab ='Sentiment',
#         main='Sentiment of Events with High Persistence',
#         col = c("red","lightblue")
#         )
# legend("topleft", c("After (A)","Before (B)"), border=c("lightblue", "red"), fill = c("lightblue", "red"))
#     text(
#       x=c(1.5, 3.5, 5.5,7.5),
#       y= c(-0.1, -0.25, 0.2, -0.1),
#       c("P-value = 3.039e-06",
#         "P-value = 3.297e-05 \n Delta = 117.17%",
#         "P-value = 0.01256 \n Delta = 101.02%",
#         "P-value = 4.832e-08"),
#       cex = 0.9,
#     )

"""### Wilcoxon tests

of sentiment within the Low and High persistence groups of tweets Before vs. After bankruptcy announcment.

values are average per day for all companies in each of the High or Low group
"""

# Commented out IPython magic to ensure Python compatibility.
# %%R
# 
# #-------------- Within High persistence ONLY--------------------------------------
# 
# # positive
# print("----------- High positive --------------")
# print(wilcox.test(
#                     b[b$Persistence_t == 'before_High', 'avg_sent_pos'], # average daily sentiment for days t-30 to t+30
#                     b[b$Persistence_t == 'after_High', 'avg_sent_pos']
#                   )
#       )
# 
# # negative
# print("----------- High negative --------------")
# print(wilcox.test(
#                     b[b$Persistence_t == 'before_High', 'avg_sent_neg'],
#                     b[b$Persistence_t == 'after_High', 'avg_sent_neg']
#                   )
#       )
# 
# # neutral
# print("----------- High neutral --------------")
# print(wilcox.test(
#                     b[b$Persistence_t == 'before_High', 'avg_sent_neu'],
#                     b[b$Persistence_t == 'after_High', 'avg_sent_neu']
#                   )
#       )
# 
# 
# # All
# print("----------- High All --------------")
# print(wilcox.test(
#                     b[b$Persistence_t == 'before_High', 'avg_sent'],
#                     b[b$Persistence_t == 'after_High', 'avg_sent']
#                   )
#       )

# Commented out IPython magic to ensure Python compatibility.
# %%R
# 
# median(b[b$Persistence_t == 'after_Low', 'avg_sent'])

# Commented out IPython magic to ensure Python compatibility.
# %%R
# 
# #-------------- Within Low persistence ONLY --------------------------------------
# 
# # positive
# print("----------- Low positive --------------")
# print(wilcox.test(
#                     b[b$Persistence_t == 'before_Low', 'avg_sent_pos'],
#                     b[b$Persistence_t == 'after_Low', 'avg_sent_pos']
#                   )
#       )
# 
# # negative
# print("----------- Low negative --------------")
# print(wilcox.test(
#                     b[b$Persistence_t == 'before_Low', 'avg_sent_neg'],
#                     b[b$Persistence_t == 'after_Low', 'avg_sent_neg']
#                   )
#       )
# 
# # neutral
# print("----------- Low neutral --------------")
# print(wilcox.test(
#                     b[b$Persistence_t == 'before_Low', 'avg_sent_neu'],
#                     b[b$Persistence_t == 'after_Low', 'avg_sent_neu']
#                   )
#       )
# 
# 
# # All
# print("----------- Low All --------------")
# print(wilcox.test(
#                     b[b$Persistence_t == 'before_Low', 'avg_sent'],
#                     b[b$Persistence_t == 'after_Low', 'avg_sent']
#                   )
#       )

"""### Test for significant **percentile** increase for High vs. Low persistence"""

# Commented out IPython magic to ensure Python compatibility.
# %%R
# #----------------------------------------- Low ---------------------------------
# # Pos: diff between before and after for Low
# diff_pos_Low  <-  (b[b$Persistence_t == 'after_Low', 'avg_sent_pos']) - (b[b$Persistence_t == 'before_Low', 'avg_sent_pos'])
# 
# # Neg: diff between before and after for Low
# diff_neg_Low  <- (b[b$Persistence_t == 'after_Low', 'avg_sent_neg']) - (b[b$Persistence_t == 'before_Low', 'avg_sent_neg'])
# 
# # Neu: diff between before and after for Low
# diff_neu_Low  <- (b[b$Persistence_t == 'after_Low', 'avg_sent_neu']) - (b[b$Persistence_t == 'before_Low', 'avg_sent_neu'])
# 
# # All: diff between before and after for Low
# diff_all_Low  <- (b[b$Persistence_t == 'after_Low', 'avg_sent'])  - (b[b$Persistence_t == 'before_Low', 'avg_sent'])
# 
# 
# # percentile
# diff_pos_Low_percent <- diff_pos_Low/b[b$Persistence_t == 'before_Low', 'avg_sent_pos']
# diff_neg_Low_percent <- diff_neg_Low/b[b$Persistence_t == 'before_Low', 'avg_sent_neg']
# diff_neu_Low_percent <- diff_pos_Low/b[b$Persistence_t == 'before_Low', 'avg_sent_neu']
# diff_all_Low_percent <- diff_all_Low/b[b$Persistence_t == 'before_Low', 'avg_sent']
# 
# #----------------------------------------- High ---------------------------------
# # Pos: diff between before and after for High
# diff_pos_High  <- (b[b$Persistence_t == 'after_High', 'avg_sent_pos']) - (b[b$Persistence_t == 'before_High', 'avg_sent_pos'])
# 
# # Neg: diff between before and after for High
# diff_neg_High  <- (b[b$Persistence_t == 'after_High', 'avg_sent_neg']) - (b[b$Persistence_t == 'before_High', 'avg_sent_neg'])
# 
# # Neu: diff between before and after for High
# diff_neu_High  <- (b[b$Persistence_t == 'after_High', 'avg_sent_neu']) - (b[b$Persistence_t == 'before_High', 'avg_sent_neu'])
# 
# # All: diff between before and after for Low
# diff_all_High  <- (b[b$Persistence_t == 'after_High', 'avg_sent']) - (b[b$Persistence_t == 'before_High', 'avg_sent'])
# 
# # percentile
# diff_pos_High_percent <- diff_pos_High/b[b$Persistence_t == 'before_High', 'avg_sent_pos']
# diff_neg_High_percent <- diff_neg_High/b[b$Persistence_t == 'before_High', 'avg_sent_neg']
# diff_neu_High_percent <- diff_neu_High/b[b$Persistence_t == 'before_High', 'avg_sent_neu']
# diff_all_High_percent <- diff_all_High/b[b$Persistence_t == 'before_High', 'avg_sent']

# Commented out IPython magic to ensure Python compatibility.
# %%R
# diff_pos_High_percent <- diff_pos_Low_percent[is.finite(diff_pos_High_percent)]
# diff_neg_High_percent <- diff_pos_Low_percent[is.finite(diff_neg_High_percent)]
# diff_neu_High_percent <- diff_pos_Low_percent[is.finite(diff_neu_High_percent)]
# diff_all_High_percent <- diff_pos_Low_percent[is.finite(diff_all_High_percent)]
#

# Commented out IPython magic to ensure Python compatibility.
# %%R
# #-------------- Wilcoxon percentile between High and Low persistence -----------
# 
# # positive
# print("----------- Pos percentile increase: LOW vs. High --------------")
# print(wilcox.test(diff_pos_Low_percent, diff_pos_High_percent))
# 
# 
# print("----------- Neg percentile increase: LOW vs. High --------------")
# print(wilcox.test(diff_neg_Low_percent, diff_neg_High_percent))
# 
# 
# print("----------- Neu percentile increase: LOW vs. High --------------")
# print(wilcox.test(diff_neu_Low_percent, diff_neu_High_percent))
# 
# 
# print("----------- All percentile increase: LOW vs. High --------------")
# print(wilcox.test(diff_all_High_percent, diff_all_High_percent))

"""#### check the means of sentiment

To discover the increase in the mean for pos, neg, neu, and all
"""

# Commented out IPython magic to ensure Python compatibility.
# %%R
# # ---- High mean sentiment----
# 
# print("---- High: mean sentiment Pos before----")
# print(mean(b[b$Persistence_t == 'before_High', 'avg_sent_pos']))
# 
# print("---- High: mean sentiment Pos after----")
# print(mean(b[b$Persistence_t == 'after_High', 'avg_sent_pos']))
# 
# print("---- High: mean sentiment Neg before----")
# print(mean(b[b$Persistence_t == 'before_High', 'avg_sent_neg']))
# 
# print("---- High: mean sentiment Neg after----")
# print(mean(b[b$Persistence_t == 'after_High', 'avg_sent_neg']))
# 
# print("---- High: mean sentiment Neu before----")
# print(mean(b[b$Persistence_t == 'before_High', 'avg_sent_neu']))
# 
# print("---- High: mean sentiment Neu after----")
# print(mean(b[b$Persistence_t == 'after_High', 'avg_sent_neu']))
# 
# 
# print("---- High: mean sentiment All before----")
# print(mean(b[b$Persistence_t == 'before_High', 'avg_sent']))
# 
# print("---- High: mean sentiment All after----")
# print(mean(b[b$Persistence_t == 'after_High', 'avg_sent']))
# 
# 
# 
# # ---- calc the delta of increas for neg, and neu
# print("---- High: delta %change in the mean before vs after sentiment for Neg ----")
# print( 100*mean(b[b$Persistence_t == 'after_High', 'avg_sent_neg']) / mean(b[b$Persistence_t == 'before_High', 'avg_sent_neg']) )
# 
# print("---- Low: delta %change in the mean before vs after sentiment for Neg ----")
# print( 100*mean(b[b$Persistence_t == 'after_Low', 'avg_sent_neg']) / mean(b[b$Persistence_t == 'before_Low', 'avg_sent_neg']) )
# 
# 
# print("---- High: delta %change in the mean before vs after sentiment for Neu ----")
# print( 100*mean(b[b$Persistence_t == 'after_High', 'avg_sent_neu']) / mean(b[b$Persistence_t == 'before_High', 'avg_sent_neu']) )
# 
# print("---- Low: delta %change in the mean before vs after sentiment for Neu ----")
# print( 100*mean(b[b$Persistence_t == 'after_Low', 'avg_sent_neu']) / mean(b[b$Persistence_t == 'before_Low', 'avg_sent_neu']) )

# Commented out IPython magic to ensure Python compatibility.
# %%R
# # ---- Low mean sentiment----
# print("---- Low: mean sentiment Pos before----")
# print(mean(b[b$Persistence_t == 'before_Low', 'avg_sent_pos']))
# 
# print("---- Low: mean sentiment Pos after----")
# print(mean(b[b$Persistence_t == 'after_Low', 'avg_sent_pos']))
# 
# 
# print("---- Low: mean sentiment Neg before----")
# print(mean(b[b$Persistence_t == 'before_Low', 'avg_sent_neg']))
# 
# print("---- Low: mean sentiment Neg after----")
# print(mean(b[b$Persistence_t == 'after_Low', 'avg_sent_neg']))
# 
# 
# 
# print("---- Low: mean sentiment Neu before----")
# print(mean(b[b$Persistence_t == 'before_Low', 'avg_sent_neu']))
# 
# print("---- Low: mean sentiment Neu after----")
# print(mean(b[b$Persistence_t == 'after_Low', 'avg_sent_neu']))
# 
# 
# 
# print("---- Low: mean sentiment All before----")
# print(mean(b[b$Persistence_t == 'before_Low', 'avg_sent']))
# 
# print("---- Low: mean sentiment All after----")
# print(mean(b[b$Persistence_t == 'after_Low', 'avg_sent']))
# 
# # The sentiment of High persistence decreased by 21.63% on average.
# # Whereas, sentiment of Low persistence decreased by 36.50% on average.
# # High persistence decreased less than Low persistence.

"""## Regression"""

# Commented out IPython magic to ensure Python compatibility.
# # set target variable
# %%R
# 
# result$y <- 0 # zero means High persistence
# result[result$company %in% forgotten_companies, 'y'] <- 1 # means Low persistence
# 
# result$company_name <- NULL
# 
# result$y <- as.factor(result$y)

# Commented out IPython magic to ensure Python compatibility.
# %%R
# 
# # Specify the proportion of data to be used for train
# train_proportion <- 0.7
# 
# # Split the data into training and test sets
# set.seed(123) # Set a random seed for reproducibility
# train_indices <- createDataPartition(y = result$y, p = train_proportion, list = FALSE)
# train_data <- result[train_indices, ]
# test_data <- result[-train_indices, ]

# Commented out IPython magic to ensure Python compatibility.
# # logistic regression
# 
# %%R
# 
# # define training control cross validation
# train_control <- trainControl(method = "cv", number = 10)
# 
# # train the model on training set
# model <- train(y ~ avg_time_from_first + avg_sentiment + avg_txt_len + Pre_announcement_mean +
#                number_of_tweets_before + Followers + Following + shrt_boost + avg_txt_len	+
#                avg_sentiment	+ avg_pos	+ avg_neg	+ avg_neu,  # Private_Public +
#                family = binomial(),
#                trControl = train_control,
#                method = "glmStepAIC",
#                direction ="backward", # forward
#                data = train_data,
#                trace = 2)

# Commented out IPython magic to ensure Python compatibility.
# %%R
# # print cv scores
# print(summary(model))
# 
# 
# # check varibale importance
# varImp(model$finalModel)
# 
# # labeling
# # 0: High persistent
# # 1: Low persistent

# Commented out IPython magic to ensure Python compatibility.
# # Evaluation of the best model of the cross validation
# 
# %%R
# 
# threshold=0.5
# predicted_values<-as.factor(ifelse(predict(model$finalModel,type="response")>threshold,1,0))
# actual_values<-as.factor(model$finalModel$y)
# confusionMatrix(predicted_values, actual_values, mode = "everything", positive="0")

# Commented out IPython magic to ensure Python compatibility.
# # Make predictions on the test set
# %%R
# 
# test_predictions <- predict(model$finalModel, newdata = test_data,type = "response")
# 
# # Set a threshold value (e.g., 0.5) for binary classification
# threshold <- 0.5
# 
# # Convert predicted probabilities to binary values based on the threshold
# test_predictions <- as.factor(ifelse(test_predictions > threshold, 1, 0))
# 
# test_data$y <- as.factor(test_data$y)
# 
# # Compute the confusion matrix
# confusionMatrix(test_predictions, test_data$y, mode = "everything", positive="0")
# 
# 
# # 0:= unforgotten companies
# # 1:= forgotten companies

"""# Other measures"""

# Commented out IPython magic to ensure Python compatibility.
# # 3) Long-term boost
# %%R
# 
# Long_term_boost = sqldf("select avg(freq) as lng_boost, company from tmp where `when`='after' and number between 7 and 40 group by company")
# Long_term_boost$shrt_boost <- Long_term_boost$lng_boost - Pre_announcement_mean$avgfreq
# 
# print(Long_term_boost)
# 
# library(DescTools)
# MedianCI(Long_term_boost$shrt_boost,
#          conf.level = 0.95,
#          na.rm = FALSE,
#          method = "exact",
#          R = 10000)

# Commented out IPython magic to ensure Python compatibility.
# %%R
# # Halving time - manual run: play with `win` variable to detect when the area under the graph (`x`) is 0.5
# # calculate AUC
# 
# comp = 'TrueReligion'
# win = 3
# x = AUC(x=tmp[tmp$when =='after'& tmp$company==comp,]$number[1:win], y=tmp[tmp$when =='after'& tmp$company==comp,]$freq[1:win]) /
#                                                                     AUC(x=tmp[tmp$when =='after'& tmp$company==comp,]$number, y=tmp[tmp$when =='after'& tmp$company==comp,]$freq)
# 
# print(x)

# Commented out IPython magic to ensure Python compatibility.
# %%R
# 
# 
# 
# halving = rbind(
#     c('beautybrands',	7),
#     c('CharlotteRusse',	5),
#     #c('CharmingCharlie',	5),
#     c('claires',	21),
#     c('DIESEL',	17),
#     c('Forever21',	7),
#     c('ftdflowers',	14),
#     c('hhgregg',	9),
#     c('MattressFirm',	14),
#     c('NineWest',	5),
#     c('Perfumania',	12),
#     c('Rockport',	11),
#     c('rue21',	15),
#     c('Sears',	4),
#     c('sugarfina',	13),
#     c('TrueReligion',	3)
# )
# 
# 
# halving = data.frame(halving)
# names(halving) = c("company","halving")
# halving

"""# Plot daily tweet count per company"""

# Commented out IPython magic to ensure Python compatibility.
# %%R
# unique(tmp$company)

# Commented out IPython magic to ensure Python compatibility.
# %%R
# # create a list of ggplots
# 
# figures <- list()
# i = 1
# 
# tmp$number <- as.numeric(tmp$number)
# tmp$freq <- as.numeric(tmp$freq)
# 
# for(comp in unique(tmp$company)){
#   figures[[i]] <- ggplot(data=tmp[tmp$company==comp,], aes(x=number, y=freq)) +
#                       geom_line()+
#                       theme_bw() +
#                       labs(x="Days from bankruptcy announcment", y = "Number of tweets")+
#                       ggtitle(paste0("@",comp)) +
#                       theme(plot.title = element_text(hjust = 0.5)) +
#                       geom_point()
# 
#   plot(figures[[i]])
#   i = i +1
# }

# Commented out IPython magic to ensure Python compatibility.
# %%R
# 
# comp <- 'A123Systems'
# PAM <- mean(tmp[tmp$company== comp & tmp$number < 0,'freq'])
# LTB <- mean(tmp[tmp$company== comp & tmp$number > 6,'freq'])
# 
# sz <- 3
# 
# i <- which(comp == unique(tmp$company))
# 
# print(i)
# 
# p1 <- figures[[i]] +  geom_text(aes(x = -17, y = PAM+6, label = "Pre-announcement \n mean"), size = sz, color="red", check_overlap = TRUE) +
#                                   geom_segment(aes(x = -30, y = PAM, xend = 0, yend = PAM), color = "red") +
#                                   geom_segment(aes(x = -1, y = PAM, xend = -1, yend = tmp[tmp$company== comp & tmp$number == 0,'freq']), color = "darkgreen", arrow = arrow(length = unit(0.2, "cm"), type = "closed")) +
#                                   geom_segment(aes(x = -1, y = tmp[tmp$company== comp & tmp$number == 0,'freq'], xend = -1, yend = PAM), color = "Blue", arrow = arrow(length = unit(0.2, "cm"), type = "closed")) +
#                                   geom_text(aes(x = -10, y = tmp[tmp$company== comp & tmp$number == 0,'freq']/2, label = "Short-term \n Boost"), size = sz, color="Blue", check_overlap = TRUE) +
#                                   geom_segment(aes(x = 6, y = LTB, xend = 30, yend = LTB), color = "Darkgreen") +
#                                   geom_text(aes(x = 20, y = LTB+5, label = "Long-term \n Boost"), size = sz, color="Darkgreen", check_overlap = TRUE)
# 
# p1

# Commented out IPython magic to ensure Python compatibility.
# %%R
# p1

# Commented out IPython magic to ensure Python compatibility.
# %%R
# # forgotten: A123Systems, SunEdison, ftdflowers, lootcrate
# # unforgotten: Avaya, ChuckECheese ,GoldsGym, Hertz, MattressFirm, cineworld,claires,revlon,rue21
# # IndonesiaGaruda, KikoMilanoUSA, LuckyBrand,NineWest, RepublicAirways, Rockport, TailorBrands, beautybrands, lordandtaylor,Abengoa,DIESEL,Golfsmith HMHCo
# 
# #
# comp <- 'IndonesiaGaruda'
# i <- which(comp == unique(tmp$company))
# print(i)
# figures[[i]]

# Commented out IPython magic to ensure Python compatibility.
# %%R
# unique(tmp$company)

# Commented out IPython magic to ensure Python compatibility.
# %%R
# 
# # plot the figures
# 
# p<-ggarrange(p1,
#              figures[[33]],
#              figures[[51]],
#              figures[[76]],
#              figures[[7]], # unforgotten: cineworld, Hertz ,Avaya, claires
#              figures[[16]],
#              figures[[17]],
#              figures[[38]],
#           ncol = 4, nrow = 2)
# options(repr.plot.width = 12, repr.plot.height = 6) # set canvas size
# print(p)
# ggexport(p, filename = "spikes.pdf", width = 12, height = 6) # write figure to pdf

"""#### plot average number of tweets before vs. after the announcement for all companies"""

# Commented out IPython magic to ensure Python compatibility.
# %%R
# # average number of tweets before and after the announcement
# fig2 <- sqldf("select number, avg(freq) as n_d from tmp group by number",method = "name__class")
# write.csv(fig2, file="fig2.csv")

# Commented out IPython magic to ensure Python compatibility.
# %%R
# p2<-ggplot(data=fig2, aes(x=number, y=n_d)) +
#                       geom_line()+
#                       #scale_y_continuous(trans='log10')+
#                       theme_bw() +
#                       labs(x="Days from bankruptcy announcment", y = "Average number of tweets")+
#                       ggtitle("Average mention time series") +
#                       theme(plot.title = element_text(hjust = 0.5)) +
#                       #geom_hline(yintercept=mean(fig2[fig2$number!=0,]$n_d), linetype="dashed", color = "red") +
#                       geom_point()
# 
# print(p2)
# 
# ggexport(p2, filename = "figures3.pdf", width = 6, height = 6) # write figure to pdf

"""# Fig. 3 Proportion of cultural vs. communicative memory

Proportion of cultural vs. communicative memory. On day t after death, the
total collective memory according to the shifted power law fit is S(t) = u(t) + v(t), where u(t) = atâˆ’b
is the
communicative memory, and v(t) = c is the cultural memory. We plot the proportion v(t)/S(t) as a function
of t. Left: news. Right: Twitter.

## clustering
"""

# Commented out IPython magic to ensure Python compatibility.
# %%R
# df_four_measures = data.frame(
#                               'Pre_announcement_mean' = Pre_announcement_mean$avgfreq,
#                               'Short_term_boost' = Short_term_boost$shrt_boost,
#                               'Long_term_boost' = Long_term_boost$lng_boost,
#                               'Halving' = as.numeric(halving$halving)
#                               )
# 
# df_four_measures

# Commented out IPython magic to ensure Python compatibility.
# %%R
# mat = as.matrix(df_four_measures)

# Commented out IPython magic to ensure Python compatibility.
# %%R
# 
# res<-NbClust(mat, distance = "euclidean", min.nc=2, max.nc=10, method = "kmeans", index = "all")
# 
# # plot
# library("factoextra")
# fviz_nbclust(res)

# Commented out IPython magic to ensure Python compatibility.
# %%R
# # do k-means
# kmeans_fancy <- kmeans(scale(mat), 2, nstart = 100)
# 
# 
# df_four_measures$company <- halving$company
# df_four_measures$kmeans <- kmeans_fancy$cluster
# df_four_measures

# Commented out IPython magic to ensure Python compatibility.
# %%R
# df_centroids = data.frame(kmeans_fancy$centers)
# df_centroids <- t(df_centroids)
# df_centroids <- data.frame(df_centroids)
# df_centroids$measure<-row.names(df_centroids)
# row.names(df_centroids)<-NULL
# 
# one <- data.frame(df_centroids[,c(1,3)])
# one$group<-"1"
# names(one)[1]<-"z-scores"
# two <- data.frame(df_centroids[,c(2,3)])
# two$group<-'2'
# names(two)[1]<-"z-scores"
# 
# df_centroids<-rbind(one, two)
# 
# # do k-means
# g<- ggplot(df_centroids, aes(measure,`z-scores`,fill=group)) +
#   geom_col()+
#   theme_bw() +
#   coord_flip()
# 
# print(g)
# ggexport(g, filename = "figures3c.pdf", width = 4, height = 4) # write figure to pdf

# Commented out IPython magic to ensure Python compatibility.
# %%R
# df_four_measures

"""## figure 3. average mention time series by group"""

# Commented out IPython magic to ensure Python compatibility.
# %%R
# tmp <- read_csv("/content/drive/MyDrive/Colab Notebooks/BIU_HEB/data/data_rebuttle.csv", show_col_types = FALSE)
# 
# # remove TailorBrands
# tmp <- tmp[!tmp$company %in% c('KikoMilanoUSA', ''),]
# 
# head(tmp[tmp$when == 'after',])

# Commented out IPython magic to ensure Python compatibility.
# %%R
# group1<-tmp[tmp$company %in% df_four_measures[df_four_measures$kmeans==1,]$company,]
# group1$group<-'C1'
# group2<-tmp[tmp$company %in% df_four_measures[df_four_measures$kmeans==2,]$company,]
# group2$group<-'C2'
# group_data <- rbind(group1,group2)
# fig3b <- sqldf("select number, `group`, avg(freq) as n_d from group_data group by number, `group`")
# fig3b$group <- as.factor(fig3b$group)
# head(fig3b)

# Commented out IPython magic to ensure Python compatibility.
# %%R
# p2<-ggplot(data=fig3b, aes(x=number, y=n_d, group=group, colour=group)) +
#                       geom_line()+
#                       theme_bw() +
#                       labs(x="Days from bankruptcy announcment", y = "Average number of tweets")+
#                       ggtitle("Average mention time series by group") +
#                       theme(plot.title = element_text(hjust = 0.5)) +
#                       geom_point()
# 
# print(p2)
# 
# ggexport(p2, filename = "figures3b.pdf", width = 6, height = 6) # write figure to pdf

# Commented out IPython magic to ensure Python compatibility.
# %%R
# boxplot(fig3b[fig3b$group=='C1',]$n_d, fig3b[fig3b$group=='C2',]$n_d)

# Commented out IPython magic to ensure Python compatibility.
# %%R
# # mean of 3 out of the 4 parameters in each cluster
# # mean(df_four_measures$Short_term_boost
# 
# #mean(df_four_measures[df_four_measures$kmeans=='1',]$Short_term_boost)
# mean(df_four_measures[df_four_measures$kmeans=='2',]$Short_term_boost)
# 
# # Short_term_boost      C1: 617.7155           C2: 107.0955            ALL  298.578
# # Long_term_boost       C1: 70.03487           C2 15.3411              ALL  35.85126
# # Pre_announcement_mean C1: 51.11              C2 16.80455             ALL  29.67202

"""## regression

1. Pre-announcement mean mention frequency.
2. Company age (year) at the day of bankruptcy announcement.
3. Bankruptcy strategy (factor with 5 levels: restructuring, reorganization, purchased, sold, closed all stores).
4. Market cup category (factor with 6 levels: specifying the main business of the company: sports, clothes, etc.
5. Market capitalization.
6. Gender of owners (factor with 3 levels: female, male, mixed).



"""

# Commented out IPython magic to ensure Python compatibility.
# %%R
# Pre_announcement_mean$avgfreq<-scale(Pre_announcement_mean$avgfreq)
# Pre_announcement_mean

# Commented out IPython magic to ensure Python compatibility.
# # average sentiment analysis of each company
# # load before and after datasets
# %%R
# 
# before <- read_csv(paste0(PTH1,"/before.csv"),col_types = cols(...1 = col_skip(), id = col_character(),
#                                                                        in_reply_to_user_id = col_character(),
#                                                                        author_id = col_character(),
#                                                                        conversation_id = col_character()))
# 
# 
# before<-before[!before$company %in% c('DeanAndDeLuca', 'TailorBrands'),]

# Commented out IPython magic to ensure Python compatibility.
# %%R
# tweets_R = before$text

# Commented out IPython magic to ensure Python compatibility.
# %R -o tweets_R tweets_R

import re
result = [re.sub(r'http\S+', '', x) for x in tweets_R]

# https://textblob.readthedocs.io/en/dev/quickstart.html#sentiment-analysis
from textblob import TextBlob

sentiment = []
subjectivity = []

for text in result:
  x = TextBlob(text)
  sentiment.append( x.sentiment[0] )
  subjectivity.append( x.sentiment[1] ) # The subjectivity is a float within the range [0.0, 1.0] where 0.0 is very objective and 1.0 is very subjective.

# Commented out IPython magic to ensure Python compatibility.
# send back sentiment to R
# %R -i sentiment sentiment
# %R -i subjectivity subjectivity

"""## T-tests"""

# Commented out IPython magic to ensure Python compatibility.
# %%R
# # reputation ratio
# cl1=c(36.01754386,
# 56.8627451,
# 132.7731092,
# 146.4516129,
# 162.406015,
# 67700)
# 
# 
# cl2=c(49.26624738,
# 85.17520216,
# 111.9669421,
# 113.8297872,
# 117.0572207,
# 144.9460647,
# 169.4719472,
# 203.6018957,
# 311.2852665,
# 74100)
# 
# 
# wilcox.test(cl1,cl2)

# Commented out IPython magic to ensure Python compatibility.
# %%R
# # difference in the number of Followers
# 
# cluster1=c(67700,
# 43200,
# 22700,
# 14500,
# 110600,
# 6159)
# 
# 
# cluster2=c(192600,
# 102700,
# 74100,
# 23500,
# 99300,
# 214800,
# 429600,
# 338700,
# 63200,
# 362800)
# 
# 
# t.test(cluster1,cluster2)

# Commented out IPython magic to ensure Python compatibility.
# %%R
# # difference in the number of lists
# 
# clst1=c(171,
# 255,
# 833,
# 155,
# 266,
# 1)
# 
# 
# clst2=c(477,
# 742,
# 3025,
# 1692,
# 1835,
# 2503,
# 606,
# 2110,
# 319,
# 1)
# 
# 
# t.test(clst1,clst2)

# Commented out IPython magic to ensure Python compatibility.
# %%R
# # list into vector
# sentiment = unlist(sentiment)
# subjectivity = unlist(subjectivity)

# Commented out IPython magic to ensure Python compatibility.
# %%R
# before$sentiment <- sentiment
# before$subjectivity <- subjectivity
# 
# mean_sent_by_comp = sqldf("select avg(sentiment), company from before group by company")
# 
# mean_subj_by_comp = sqldf("select avg(subjectivity), company from before group by company")
# 
# 
# mean_sent_by_comp<-mean_sent_by_comp[!mean_sent_by_comp$company %in% c('KikoMilanoUSA', ''),]
# mean_subj_by_comp<-mean_subj_by_comp[!mean_subj_by_comp$company %in% c('KikoMilanoUSA', ''),]

# Commented out IPython magic to ensure Python compatibility.
# %%R
# # number of tweets before
# num_of_tweets_before = sqldf("select company, count(1) as n_tweets from before group by company")
# num_of_tweets_before = num_of_tweets_before[num_of_tweets_before$company != "KikoMilanoUSA",]
# num_of_tweets_before

# Commented out IPython magic to ensure Python compatibility.
# %%R
# unique(before$company)

# Commented out IPython magic to ensure Python compatibility.
# %%R
# # calculate mean inter-tweet time
# inter_tweet_time = data.frame()
# for(company in unique(before$company)){
#     v = as.POSIXct( before[before$company==company,]$created_at , format = "%a %b %d %H:%M:%S %z %Y", tz="GMT")
#     v = v[order(v , decreasing = TRUE )]
#     d = diff(v)
#     inter_tweet_time = rbind(inter_tweet_time, c(company, mean(d)))
# }
# 
# names(inter_tweet_time) = c('company','avg_inter_time')
# #inter_tweet_time = inter_tweet_time[order(num_of_tweets_before$company),]
# inter_tweet_time = sqldf("select company, avg_inter_time/60 as avg_inter_time from inter_tweet_time group by company") # we do this to keep the order of companies
# inter_tweet_time <- inter_tweet_time[inter_tweet_time$company != 'KikoMilanoUSA',]
# inter_tweet_time

# Commented out IPython magic to ensure Python compatibility.
# %%R
# 
# Company_type = c( 'Beauty',
#                   'Clothing',
#                   'Clothing',
#                   'Department Store',
#                   'Clothing',
#                   'Clothing',
#                   'Other',
#                   'Department Store',
#                   #'Beauty',
#                   'Mattresses',
#                   'Clothing',
#                   'Beauty',
#                   'Clothing',
#                   'Other',
#                   'Department Store',
#                   'Other',
#                   'Clothing'
#                   )
# 
# Followers = c(8403,
#               72100,
#               28500,
#               65400,
#               179000,
#               1900000,
#               12800,
#               31800,
#               #4367,
#               47200,
#               45400,
#               19000,
#               6294,
#               32400,
#               198100,
#               22300,
#               44600)
# 
# 
# Following = c(
#                 707,
#                 9438,
#                 148,
#                 225,
#                 737,
#                 351,
#                 21,
#                 900,
#                 #47,
#                 554,
#                 387,
#                 159,
#                 940,
#                 1091,
#                 9293,
#                 172,
#                 193
#                 )
# 
# # mean web searched worldwide 1 month before the announcedment on Google Trends
# Google_trends = c(
#                   69.84,
#                   74.9,
#                   59.45,
#                   35.86,
#                   84.86,
#                   54.93,
#                   25.31,
#                   50.06,
#                   #
#                   30.77,
#                   64.93,
#                   62.46,
#                   87.38,
#                   65.19,
#                   28,
#                   32.75,
#                   11.51
#                   )
# 
# public = c( 'Private',
#             'Private',
#             'Private',
#             'Private',
#             'Private',
#             'Private',
#             'Public',
#             'Public',
#             #'Private',
#             'Private',
#             'Private',
#             'Public',
#             'Private',
#             'Private',
#             'Public',
#             'Private',
#             'Private')

# Commented out IPython magic to ensure Python compatibility.
# %%R
# 
# df = data.frame(
#                   'company' = Pre_announcement_mean$company,
#                   'Pre_announcement_mean' = Pre_announcement_mean$avgfreq,
#                   'Company_type' = as.factor(Company_type),
#                   'Short_term_boost' = Short_term_boost$shrt_boost,
#                   'Long_term_boost' = Long_term_boost$shrt_boost,
#                   'avg_inter_time' = inter_tweet_time$avg_inter_time,
#                   'num_of_tweets_before' = num_of_tweets_before$n_tweets,
#                   'Followers' = Followers,
#                   'Following' = Following,
#                   'Google_trends' = Google_trends,
#                   'public' = as.factor(public)
#                 )

# Commented out IPython magic to ensure Python compatibility.
# %%R
# length(num_of_tweets_before$n_tweets)

# Commented out IPython magic to ensure Python compatibility.
# # *** predicct short-term boost ***
# %%R
# 
# df$Company_type <- relevel(df$Company_type, ref = 'Clothing') # Clothing
# df$public <- relevel(df$public, ref = 'Private')
# 
# 
# library(MASS)
# # Fit the full model
# full.model <-lm(df$Short_term_boost ~
#                                 df$Pre_announcement_mean +
#                                 #df$mean_sent_by_comp +
#                                 #df$avg_inter_time +
#                                 df$Company_type +
#                                 df$Google_trends +
#                                 df$num_of_tweets_before +
#                                 df$public
#                                 #+df$Followers+
#                                 #df$Following +
#                                 #df$mean_subj_by_comp
#                 )
# 
# # Stepwise regression model
# step.model <- stepAIC(full.model, direction = "both",
#                       trace = FALSE)
# summary(step.model)

# Commented out IPython magic to ensure Python compatibility.
# # *** predict long-term boost ***
# %%R
# model2<-lm(log(Long_term_boost) ~     Pre_announcement_mean +
#                                       #as.numeric(avg_inter_time) +
#                                       Followers +
#                                       Short_term_boost,
#                                     data = df
#                                       )
# 
# 
# summary(model2)

# Commented out IPython magic to ensure Python compatibility.
# %%R
# (df$Long_term_boost)

# Commented out IPython magic to ensure Python compatibility.
# %%R
# names(df)